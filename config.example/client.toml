# MCP Client Configuration Template
#
# Copy this file to config/client.toml and customize for your setup.
# See README.md for detailed configuration guide.
#
# NOTE: Model settings (default_provider, model, prompt_template, tools)
# are now in a separate file: model.toml

# =============================================================================
# PROVIDERS - Configure your LLM backends
# =============================================================================

# Cloud/External API Provider
# Supported types: gemini, openai, anthropic, etc.
[[providers]]
id = "cloud"
type = "gemini"                                      # Provider type
endpoint = "https://api.example.com"                 # API endpoint
api_key = "API_KEY_ENV_VAR"                          # Environment variable name
models = [
    { name = "default-model", display_name = "Default Model" },
    { name = "fast-model", display_name = "Fast Model" },
    { name = "pro-model", display_name = "Pro Model" },
]

# Local Provider (e.g., Ollama)
[[providers]]
id = "local"
type = "ollama"
endpoint = "http://127.0.0.1:11434"
models = [
    { name = "llama3", display_name = "Llama 3" },
    { name = "mistral", display_name = "Mistral" },
]

# =============================================================================
# REST SERVER - CORS and API documentation
# =============================================================================

[server]
# CORS allowed origins (leave empty for permissive)
cors_origins = [
    "http://localhost:5173",
    "http://127.0.0.1:5173",
]

# API documentation servers (shown in Swagger UI)
[[server.docs]]
url = "http://localhost:8080"
description = "Local development"

# =============================================================================
# MCP SERVERS - Define your tool servers
# =============================================================================

# Example server configuration:
#
# [[servers]]
# name = "example"                    # Unique identifier
# command = "/path/to/server-binary"  # Path to executable
# args = ["--flag", "value"]          # Optional arguments
# workdir = "/working/directory"      # Optional working directory
# env = { KEY = "value" }             # Optional environment variables
